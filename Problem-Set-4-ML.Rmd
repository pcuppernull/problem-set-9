---
title: "Problem Set 4"
author: "Pete Cuppernull"
date: "2/27/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(seriation)
library(skimr)
library(dendextend) 
library(mixtools)
library(plotGMM)
library(scales)
```

K-Means by hand
```{r}
set.seed(1414)
x <- cbind(c(1, 1, 0, 5, 6, 4), c(4, 3, 4, 1, 2, 0))


#Plot the observations.
plot(x)

# Randomly assign a cluster label to each observation. Report the cluster labels for each observation and plot the results with a different color for each cluster (remember to set your seed first).
colnames(x) <- c("x", "y")
label <- as.factor(sample(1:2, 6, replace=TRUE))
x <- cbind(x, label)

#compute the centroid for each cluster.
x_df <- as.data.frame(x)

centroids <- x_df %>%
  group_by(label) %>%
  mutate(mean(x), mean(y)) %>%
  select(`mean(x)`, `mean(y)`) %>%
  distinct()


#Assign each observation to the centroid to which it is closest, in terms of Euclidean distance. Report the cluster labels for each observation.
x1 <- centroids[1,2]
x2 <- centroids[2,2]
y1 <- centroids[1,3]
y2 <- centroids[2,3]
xs <- cbind(x1, x2) 
ys <- cbind(y1, y2) 
points <- cbind(xs, ys)
colnames(points) <- c("x1", "x2", "y1", "y2")

df_cluster <- as.data.frame(cbind(x, points))


df_cluster %>%
  mutate(new_label = if_else(  ((abs(x-x1) + abs(y-y1)) / 2) < ((abs(x-x2) + abs(y-y2)) / 2), 1, 2)) %>%
  select(x, y, new_label)

#Repeat (3) and (4) until the answers/clusters stop changing.
iterate <- function(original_df){
  x_df <- as.data.frame(original_df)

centroids <- x_df %>%
  group_by(label) %>%
  mutate(mean(x), mean(y)) %>%
  select(`mean(x)`, `mean(y)`) %>%
  distinct()
  
x1 <- centroids[1,2]
x2 <- centroids[2,2]
y1 <- centroids[1,3]
y2 <- centroids[2,3]
xs <- cbind(x1, x2) 
ys <- cbind(y1, y2) 
points <- cbind(xs, ys)
colnames(points) <- c("x1", "x2", "y1", "y2")

df_cluster <- as.data.frame(cbind(x, points))
  
new_df <- df_cluster %>%
  mutate(label = if_else(  ((abs(x-x1) + abs(y-y1)) / 2) < ((abs(x-x2) + abs(y-y2)) / 2), 1, 2)) %>%
  select(x, y, label)

new_df
}

#Now, manually iterate until convergence
##Stage 1
x2 <- iterate(x)
x == x2 #Not converged
x3 <- iterate(x2)

x2 == x3 ##converged!!

x4 <- iterate(x3)

x3 == x4

#Reproduce the original plot from (1), but this time color the observations according to the clusters labels you obtained by iterating the cluster centroid calculation and assignments.
final_centroids <- as.data.frame(x4) %>%
  group_by(label) %>%
  mutate(mean(x), mean(y)) %>%
  select(`mean(x)`, `mean(y)`) %>%
  distinct()

ggplot() +
  geom_point(data = as.data.frame(x4), mapping = aes(x, y, color = as.factor(label))) +
  geom_point(data = final_centroids, mapping = aes(`mean(x)`, `mean(y)`, color = as.factor(label)), size = 5, alpha = .4)
  
  

```


##Clustering State Legislative Professionalism

Munge Data
```{r}
data <- load("Data and Codebook/legprof-components.v1.0.RData")

data <- x

predictors <- data %>%
  select(stateabv, t_slength, slength, salary_real, expend, year) %>%
  filter(year == 2010) %>%
  na.omit() %>%
  mutate(t_slength = scale(t_slength),
         slength = scale(slength),
         salary_real = scale(salary_real),
         expend = scale(expend)) %>%
  select(-stateabv, -year)

states <- data %>%
  select(stateabv, t_slength, slength, salary_real, expend, year) %>%
  filter(year == 2010) %>%
  na.omit() %>%
  select(stateabv)

data_clean <- data %>%
  select(stateabv, t_slength, slength, salary_real, expend, year) %>%
  filter(year == 2010) %>%
  na.omit()  %>%
  select(-stateabv, -year)


data_clean_ahc <- data %>%
  select(stateabv, t_slength, slength, salary_real, expend, year) %>%
  filter(year == 2010) %>%
  na.omit()  %>%
  select(-year)

```

Assess Clusterability
```{r}
predictors
predictors2 <- predictors[sample(seq_len(nrow(predictors))),]

predictor_distance <- dist(as.matrix(predictors2), method = "euclidean")

## PLOT MATRICES
#Method 1
dissplot(predictor_distance,
         method = "HC_average",
         options = list(main = "Dissimilarity plot with seriation"))
#Method 2
dissplot(predictor_distance,
         method = "OLO_single",
         options = list(main = "Dissimilarity plot with seriation"))
#Method 3
dissplot(predictor_distance,
         method = "VAT",
         options = list(main = "Dissimilarity plot with seriation"))
```

Although a little cloudy, there appears to be stratification and blocking in the dissimilarity plots, indicating a nonrandom structure to the data.

Agglomerative Hierarchical
```{r}

par(mfrow = c(2,2))
hc_single <- hclust(predictor_distance, 
                    method = "single"); plot(hc_single, hang = -1)

hc_complete <- hclust(predictor_distance, 
                      method = "complete"); plot(hc_complete, hang = -1)

hc_average <- hclust(predictor_distance, 
                     method = "average"); plot(hc_average, hang = -1)

hc_centroid <- hclust(predictor_distance,
                      method = "centroid"); plot(hc_centroid, hang = -1)

# reset plot space
par(mfrow = c(1,1))


#lets cut up the trees and see how many clusters we might have
cuts_single <- cutree(hc_single, 
               k = c(2,3))
cuts_comp <- cutree(hc_complete, 
               k = c(2,3))
cuts_avg <- cutree(hc_average, 
               k = c(2,3))
cuts_cent <- cutree(hc_centroid, 
               k = c(2,3))


### make tables
table(`2 Clusters` = cuts_single[,1],
  `3 Clusters` = cuts_single[,2])
table(`2 Clusters` = cuts_comp[,1],
  `3 Clusters` = cuts_comp[,2])
table(`2 Clusters` = cuts_avg[,1],
  `3 Clusters` = cuts_avg[,2])
table(`2 Clusters` = cuts_cent[,1],
  `3 Clusters` = cuts_cent[,2])

```

At a high level, we see the most clustering in the complete model, where the data are split about 80/20 between two clusters. In all the models, moving from 2 to 3 clusters moves only one state into the third cluster.

K-means
```{r}
kmeans <- kmeans(predictors, 
                 centers = 2,
                 nstart = 15)
# Inspect the kmeans object
str(kmeans)

# Or call individual values, such as...
kmeans$cluster
kmeans$centers
kmeans$size

predictors$Cluster <- as.factor(kmeans$cluster) # save clusters as factor for plotting

# Assess a little more descriptively
t <- as.table(kmeans$cluster)
(t <- data.frame(t))
rownames(t) <- states$stateabv
colnames(t)[colnames(t)=="Freq"] <- "Assignment"
t$Var1 <- NULL

kmeans_results <- t

```
The K-means algorithm with two clusters spearated 6 observations out from the rest of the data -- this is somewhat consistent with the AHC model above, whiich also created two groups of disproportional size. The sttates that appear in thissecond group appear to be higher income, higher population states, such as California, New York, and Massachusetts.

GMM
```{r}
gmm1 <- mvnormalmixEM(predictors[,-5], k = 2) # fit the GMM using EM and 2 comps

gmm_results <- as.data.frame(gmm1$posterior) %>%
  cbind(states) %>%
  mutate(assignment = if_else(comp.1 < comp.2, 2, 1)) %>%
  select(stateabv, assignment)

gmm_results %>%
  count(assignment) 

gmm_results
```

Similarly, we have a small group of similar states that are separated out into a smaller cluster.  After this stage, I have a higher degree of confidence that the models are picking up nonrandom variation among this subset of states.

Compare Results

Graphically
```{r}
#Salary and Expenditures
#AHC
ahc_cuts <- as.data.frame(cuts_comp[,1]) %>%
  add_rownames(var = "statenum")

data_clean_ahc %>%
  add_rownames(var = "statenum") %>%
  left_join(ahc_cuts) %>%
  mutate(`Cluster Assignment` = as.factor(`cuts_comp[, 1]`)) %>%
  na.omit() %>%
  ggplot() +
  geom_point(aes(salary_real, expend, color = `Cluster Assignment`)) +
  theme_bw() +
  geom_text(aes(salary_real, expend, label=stateabv),hjust=-.2, vjust=-.2, size = 3) +
  labs(title = "AHC Classification",
       subtitle = "By Salary and Expenditures, in thousands of USD",
       x = "Salary",
       y = "Expenditures")

##K-Means
kmeans_results %>%
  cbind(data_clean) %>%
  add_rownames(var = "stateabv") %>%
  mutate(`Cluster Assignment` = as.factor(Assignment)) %>%
  ggplot() +
  geom_point(aes(salary_real, expend, color = `Cluster Assignment`)) +
  theme_bw() +
  geom_text(aes(salary_real, expend, label=stateabv),hjust=-.2, vjust=-.2, size = 3) +
  labs(title = "K-Means Classification",
       subtitle = "By Salary and Expenditures, in thousands of USD",
       x = "Salary",
       y = "Expenditures")

##GMM
gmm_results %>%
  cbind(data_clean) %>%
  mutate(`Cluster Assignment` = as.factor(assignment)) %>%
  ggplot() +
  geom_point(aes(salary_real, expend, color = `Cluster Assignment`)) +
  theme_bw() +
  geom_text(aes(salary_real, expend, label=stateabv),hjust=-.2, vjust=-.2, size = 3) +
  labs(title = "GMM Classification",
       subtitle = "By Salary and Expenditures, in thousands of USD",
       x = "Salary",
       y = "Expenditures")

```

Another Graph
```{r}
##AHC
data_clean_ahc %>%
  add_rownames(var = "statenum") %>%
  left_join(ahc_cuts) %>%
  mutate(`Cluster Assignment` = as.factor(`cuts_comp[, 1]`)) %>%
  na.omit() %>%
  ggplot() +
  geom_histogram(aes(t_slength, fill = `Cluster Assignment`)) +
  theme_bw() +
   labs(title = "AHC Classification",
       subtitle = "By Total Session Length",
       x = "Session Length (Days)",
       y = "Frequency") +
  scale_y_continuous(breaks= pretty_breaks())

##K-Means
kmeans_results %>%
  cbind(data_clean) %>%
  add_rownames(var = "stateabv") %>%
  mutate(`Cluster Assignment` = as.factor(Assignment)) %>%
  ggplot() +
  geom_histogram(aes(t_slength, fill = `Cluster Assignment`)) +
  theme_bw() +
   labs(title = "K-Means Classification",
       subtitle = "By Total Session Length",
       x = "Session Length (Days)",
       y = "Frequency") +
  scale_y_continuous(breaks= pretty_breaks())

##GMM
gmm_results %>%
  cbind(data_clean) %>%
  mutate(`Cluster Assignment` = as.factor(assignment)) %>%
  ggplot() +
  geom_histogram(aes(t_slength, fill = `Cluster Assignment`)) +
  theme_bw() +
   labs(title = "GMM Classification",
       subtitle = "By Total Session Length",
       x = "Session Length (Days)",
       y = "Frequency") +
  scale_y_continuous(breaks= pretty_breaks())

```


